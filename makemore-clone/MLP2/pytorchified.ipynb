{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Names.txt\", 'r') as f:\n",
    "    names = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(list(''.join(names))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopchars = [\" \",\"-\",\".\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [name.lower() for name in names if not any(char in stopchars for char in name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = np.unique(['!']+list(''.join(names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_index = dict()\n",
    "for char in chars:\n",
    "    char_index[char] = len(char_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length = 8\n",
    "Xs,ys = [],[]\n",
    "for name in names:\n",
    "    window = [0]*window_length\n",
    "    for c in name+'!':\n",
    "        Xs.append(window)\n",
    "        ys.append(char_index[c])\n",
    "        window = window[1:]+[char_index[c]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = torch.tensor(Xs, dtype=torch.long, device=device)\n",
    "ys = torch.tensor(ys, dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(30):\n",
    "    print(chars[Xs[i].cpu()], chars[ys[i].cpu()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs.shape, Xs.dtype, ys.shape, ys.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    def __init__(self, fan_in, fan_out, bias=True, device=\"cpu\"):\n",
    "        self.weight = torch.randn((fan_in, fan_out), device=device) / fan_in**0.5\n",
    "        self.bias = torch.zeros(fan_out, device=device) if bias else None\n",
    "    def __call__(self,X):\n",
    "        self.out = X @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([self.bias] if self.bias is not None else [])\n",
    "\n",
    "class BatchNorm1d():\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.001, device=\"cpu\"):\n",
    "        self.gamma = torch.ones((1,dim), device=device)\n",
    "        self.beta = torch.zeros((1,dim), device=device)\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.running_mean = torch.zeros((1,dim), device=device)\n",
    "        self.running_var = torch.ones((1,dim), device=device)\n",
    "        self.training = True\n",
    "    def __call__(self, X):\n",
    "        if self.training:\n",
    "            with torch.inference_mode():\n",
    "                mean = X.mean(0, keepdim=True)\n",
    "                var = X.var(0, keepdim=True)\n",
    "                self.running_mean = (1-self.momentum) * self.running_mean + self.momentum * mean\n",
    "                self.running_var = (1-self.momentum) * self.running_var + self.momentum * var\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "        return self.gamma * (X - mean) / (torch.sqrt(var)+self.eps) + self.beta\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "class Embedding():\n",
    "    def __init__(self, num_embeddings, dim, device=\"cpu\"):\n",
    "        self.weight = torch.randn((num_embeddings, dim), device=device)\n",
    "    def __call__(self, ix):\n",
    "        self.out = self.weight[ix]\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return [self.weight]\n",
    "\n",
    "class Flatten():\n",
    "    def __call__(self, X):\n",
    "        self.out = X.reshape(X.shape[0],-1)\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Tanh():\n",
    "    def __call__(self, X):\n",
    "        self.out = torch.tanh(X)\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Sequential():\n",
    "    def __init__(self, layers=[]):\n",
    "        self.layers=layers\n",
    "    def __call__(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "        return X\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 15\n",
    "hidden_layer = 100\n",
    "# window_length = 8\n",
    "l = window_length*embedding_dim\n",
    "model = Sequential([\n",
    "    Embedding(len(chars), embedding_dim, device=device),\n",
    "    Flatten(),\n",
    "    Linear(l,hidden_layer,bias=False, device=device),\n",
    "    BatchNorm1d(hidden_layer, device=device),\n",
    "    Tanh(),\n",
    "    Linear(hidden_layer,len(chars), device=device),\n",
    "])\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True\n",
    "print(sum(p.nelement() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lre = np.linspace(-3,0,1000)\n",
    "lrs = 10**lre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs, ys, test_size=0.1, random_state=42)\n",
    "X_train,X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "exp_rates = []\n",
    "losses = []\n",
    "for i in range(1000):\n",
    "    #minibatch\n",
    "    batch = torch.randint(0,len(X_train), (batch_size,))\n",
    "    x_batch=X_train[batch]\n",
    "    y_batch=y_train[batch]\n",
    "    # forward pass\n",
    "    logits = model(x_batch)\n",
    "    loss = F.cross_entropy(logits, y_batch)\n",
    "    # backward pass\n",
    "    for p in model.parameters():\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # update parameters\n",
    "    for p in model.parameters():\n",
    "        p.data -= lrs[i] * p.grad\n",
    "    losses.append(loss.item())\n",
    "    exp_rates.append(lre[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(exp_rates,losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10**--1 = 0.1 seems to be a good learning rate\n",
    "batch_size=32\n",
    "losses = []\n",
    "epochs = 200000\n",
    "for i in range(epochs):\n",
    "    #minibatch\n",
    "    batch = torch.randint(0,len(X_train), (batch_size,))\n",
    "    x_batch=X_train[batch]\n",
    "    y_batch=y_train[batch]\n",
    "    # forward pass\n",
    "    logits = model(x_batch)\n",
    "    loss = F.cross_entropy(logits, y_batch)\n",
    "    # backward pass\n",
    "    for p in model.parameters():\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # update parameters\n",
    "    lr = 0.1 if i < 100000 else 0.01\n",
    "    for p in model.parameters():\n",
    "        p.data -= lr * p.grad\n",
    "    # print(loss.item())\n",
    "    losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = np.array(losses)\n",
    "plt.plot(losses.reshape(-1,1000).mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.training = False\n",
    "with torch.inference_mode():\n",
    "    logits = model(X_train)\n",
    "    train_loss = F.cross_entropy(logits, y_train)\n",
    "    x = X_val\n",
    "    logits = model(x)\n",
    "    val_loss = F.cross_entropy(logits, y_val)\n",
    "    print(f'Train loss: {train_loss.item()}, Val loss: {val_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "torch.save(model, 'params/model.pt')\n",
    "with open('params/chars.pkl', 'wb') as f:\n",
    "    pickle.dump(chars, f)\n",
    "with open('params/char_index.pkl', 'wb') as f:\n",
    "    pickle.dump(char_index, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11f35b3a2c2f9db604914e925818b5f3881c103430a3a0a48ead856bc8f67805"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
